{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf77d37f",
   "metadata": {},
   "source": [
    "Corroborar si este es mas rapido al no seleccionar días separados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e40818d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import sqlite3\n",
    "# import pandas as pd\n",
    "# import cdsapi\n",
    "# import zipfile\n",
    "# import tempfile\n",
    "# from netCDF4 import Dataset, num2date\n",
    "\n",
    "# # ————— Configuración de prototipo —————\n",
    "# DB_PATH   = r\"C:\\Users\\Nainh\\Proton Drive\\nainho1306\\My files\\Licitacion\\Plataforma local\\backend\\data\\mi_base_de_datos5.db\"\n",
    "# TMP_DIR   = \"tmp_test_inc\"\n",
    "# CDS_URL   = 'https://cds.climate.copernicus.eu/api'\n",
    "# CDS_KEY   = 'd9af180c-f7f8-4a2e-8029-09018fb8c920'\n",
    "# AREA      = [-32.0, -63.0, -33.0, -61.0]\n",
    "# VARIABLES = ['2m_temperature','2m_dewpoint_temperature','total_precipitation']\n",
    "\n",
    "# os.makedirs(TMP_DIR, exist_ok=True)\n",
    "\n",
    "# # ————— Conectar y asegurar tabla —————\n",
    "# conn = sqlite3.connect(DB_PATH)\n",
    "# cur  = conn.cursor()\n",
    "# cur.execute(\"\"\"\n",
    "# CREATE TABLE IF NOT EXISTS climate_test (\n",
    "#     date TEXT,\n",
    "#     latitude REAL, longitude REAL,\n",
    "#     t2m REAL, d2m REAL, tp REAL,\n",
    "#     PRIMARY KEY(date, latitude, longitude)\n",
    "# )\n",
    "# \"\"\")\n",
    "# conn.commit()\n",
    "\n",
    "# # ————— Determinar rango de fechas faltantes —————\n",
    "# # 1) Consultar la última fecha\n",
    "# cur.execute(\"SELECT MAX(date) FROM climate_test\")\n",
    "# last = cur.fetchone()[0]  # e.g. '2025-06-15' o None\n",
    "\n",
    "# if last is None:\n",
    "#     # si no hay datos, arrancar 6 meses atrás\n",
    "#     start = pd.to_datetime(\"today\").normalize() - pd.DateOffset(months=6)\n",
    "# else:\n",
    "#     # arrancar un día después de la última fecha\n",
    "#     start = pd.to_datetime(last) + pd.Timedelta(days=1)\n",
    "\n",
    "# # hasta ayer\n",
    "# end = pd.to_datetime(\"today\").normalize() - pd.Timedelta(days=1)\n",
    "\n",
    "# if start > end:\n",
    "#     print(\"La base ya está al día (hasta\", end.date(), \")\")\n",
    "#     conn.close()\n",
    "#     exit()\n",
    "\n",
    "# dates = pd.date_range(start, end, freq=\"D\")\n",
    "# print(f\"Faltan datos de {start.date()} a {end.date()} ({len(dates)} días)\")\n",
    "\n",
    "# # ————— Función de ingesta (idéntica) —————\n",
    "# def ingest_from_zip(table_name, zip_filepath):\n",
    "#     extracted = []\n",
    "#     with zipfile.ZipFile(zip_filepath, 'r') as z:\n",
    "#         nc_members = [m for m in z.namelist() if m.endswith('.nc')]\n",
    "#         for member in nc_members:\n",
    "#             tmpf = tempfile.NamedTemporaryFile(suffix=\".nc\", delete=False)\n",
    "#             tmpf.write(z.read(member)); tmpf.close()\n",
    "#             extracted.append(tmpf.name)\n",
    "\n",
    "#     # leer dimensiones y variables\n",
    "#     data = {}\n",
    "#     times = lats = lons = None\n",
    "#     for path in extracted:\n",
    "#         ds = Dataset(path)\n",
    "#         if times is None:\n",
    "#             tvar = next(v for v in ds.variables if \"time\" in v.lower())\n",
    "#             latv = next(v for v in ds.variables if \"lat\"  in v.lower())\n",
    "#             lonv = next(v for v in ds.variables if \"lon\"  in v.lower())\n",
    "#             times = num2date(ds.variables[tvar][:],\n",
    "#                              ds.variables[tvar].units,\n",
    "#                              getattr(ds.variables[tvar],'calendar','standard'))\n",
    "#             lats  = ds.variables[latv][:]\n",
    "#             lons  = ds.variables[lonv][:]\n",
    "#         # la variable restante\n",
    "#         vars_ = [v for v in ds.variables if v not in (tvar, latv, lonv, \"number\")]\n",
    "#         data[vars_[0]] = ds.variables[vars_[0]][:]\n",
    "#         ds.close()\n",
    "\n",
    "#     sql = f\"\"\"INSERT OR REPLACE INTO {table_name}\n",
    "#               (date, latitude, longitude, t2m, d2m, tp)\n",
    "#               VALUES (?,?,?,?,?,?)\"\"\"\n",
    "#     cnt = 0\n",
    "#     for ti, dt in enumerate(times):\n",
    "#         dstr = dt.strftime(\"%Y-%m-%d\")\n",
    "#         for yi, lat in enumerate(lats):\n",
    "#             for xi, lon in enumerate(lons):\n",
    "#                 v2m = float(data.get(\"2m_temperature\", data.get(\"t2m\"))[ti,yi,xi])\n",
    "#                 vdm = float(data.get(\"2m_dewpoint_temperature\", data.get(\"d2m\"))[ti,yi,xi])\n",
    "#                 tp  = float(data.get(\"total_precipitation\", data.get(\"tp\"))[ti,yi,xi])\n",
    "#                 cur.execute(sql, (dstr, float(lat), float(lon), v2m, vdm, tp))\n",
    "#                 cnt += 1\n",
    "#     conn.commit()\n",
    "\n",
    "#     # limpiar\n",
    "#     for f in extracted + [zip_filepath]:\n",
    "#         try: os.remove(f)\n",
    "#         except: pass\n",
    "#     return cnt\n",
    "\n",
    "# # ————— Descarga y proceso por mes —————\n",
    "# c = cdsapi.Client(url=CDS_URL, key=CDS_KEY)\n",
    "# for period, group in dates.to_series().groupby(dates.to_series().dt.to_period('M')):\n",
    "#     year  = str(period.year)\n",
    "#     month = f\"{period.month:02d}\"\n",
    "#     days  = [f\"{d.day:02d}\" for d in group]\n",
    "\n",
    "#     zip_path = os.path.join(TMP_DIR, f\"{year}{month}.zip\")\n",
    "#     print(f\"-> {year}-{month}: días {days}\")\n",
    "\n",
    "#     c.retrieve(\n",
    "#         'derived-era5-single-levels-daily-statistics',\n",
    "#         {\n",
    "#             'product_type':    'reanalysis',\n",
    "#             'format':          'netcdf',\n",
    "#             'variable':        VARIABLES,\n",
    "#             'year':            [year],\n",
    "#             'month':           [month],\n",
    "#             'day':             days,\n",
    "#             'daily_statistic': 'daily_mean',\n",
    "#             'frequency':       '6_hourly',\n",
    "#             'time_zone':       'utc+00:00',\n",
    "#             'area':            AREA,\n",
    "#         },\n",
    "#         zip_path\n",
    "#     )\n",
    "\n",
    "#     n = ingest_from_zip(\"climate_test\", zip_path)\n",
    "#     print(f\"   ✔ Insertadas/actualizadas {n} filas ({year}-{month})\")\n",
    "\n",
    "# conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f9b8d341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descargando datos de 2025-07-01 a 2025-07-20 (20 días)…\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-21 16:32:23,298 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2025-07-21 16:32:24,174 INFO Request ID is 4eed9029-0fb8-43da-8d04-8eafa0fa45a9\n",
      "2025-07-21 16:32:24,259 INFO status has been updated to accepted\n",
      "2025-07-21 16:32:38,312 INFO status has been updated to running\n",
      "2025-07-21 16:33:40,344 INFO status has been updated to successful\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ed2f1cacfda4159aedadd79cde96bcb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "adfff12129898cc4ca364436f4cf98bb.zip:   0%|          | 0.00/67.9k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Insertadas/actualizadas 675 filas en climate_test\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import cdsapi\n",
    "import zipfile\n",
    "from netCDF4 import Dataset, num2date\n",
    "\n",
    "# ————— Configuración de prototipo —————\n",
    "DB_PATH   = r\"C:\\Users\\Nainh\\Proton Drive\\nainho1306\\My files\\Licitacion\\Plataforma local\\backend\\data\\mi_base_de_datos5.db\"\n",
    "TMP_DIR   = \"tmp_test20d\"\n",
    "CDS_URL   = 'https://cds.climate.copernicus.eu/api'\n",
    "CDS_KEY   = 'd9af180c-f7f8-4a2e-8029-09018fb8c920'\n",
    "AREA      = [-32.0, -63.0, -33.0, -61.0]  # [Norte, Oeste, Sur, Este]\n",
    "VARIABLES = ['2m_temperature','2m_dewpoint_temperature','total_precipitation']\n",
    "\n",
    "os.makedirs(TMP_DIR, exist_ok=True)\n",
    "\n",
    "# ————— Prepara SQLite y tabla de prueba —————\n",
    "conn = sqlite3.connect(DB_PATH)\n",
    "cur  = conn.cursor()\n",
    "cur.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS climate_test (\n",
    "    date TEXT,\n",
    "    latitude REAL,\n",
    "    longitude REAL,\n",
    "    t2m REAL,\n",
    "    d2m REAL,\n",
    "    tp REAL,\n",
    "    PRIMARY KEY(date, latitude, longitude)\n",
    ")\n",
    "\"\"\")\n",
    "conn.commit()\n",
    "\n",
    "# ————— Calcula últimos 20 días —————\n",
    "end   = pd.to_datetime(\"today\").normalize() - pd.Timedelta(days=1)\n",
    "start = end - pd.Timedelta(days=19)\n",
    "dates = pd.date_range(start, end, freq=\"D\")\n",
    "years  = sorted({str(d.year)     for d in dates})\n",
    "months = sorted({f\"{d.month:02d}\" for d in dates})\n",
    "days   = sorted({f\"{d.day:02d}\"   for d in dates})\n",
    "\n",
    "print(f\"Descargando datos de {start.date()} a {end.date()} ({len(dates)} días)…\")\n",
    "\n",
    "# ————— Descarga del ZIP con los 3 *.nc —————\n",
    "zip_path = os.path.join(TMP_DIR, \"test20d.zip\")\n",
    "c = cdsapi.Client(url=CDS_URL, key=CDS_KEY)\n",
    "c.retrieve(\n",
    "    'derived-era5-single-levels-daily-statistics',\n",
    "    {\n",
    "        'product_type':    'reanalysis',\n",
    "        'format':          'netcdf',       # el API nos devuelve un ZIP\n",
    "        'variable':        VARIABLES,\n",
    "        'year':            years,\n",
    "        'month':           months,\n",
    "        'day':             days,\n",
    "        'daily_statistic': 'daily_mean',\n",
    "        'frequency':       '6_hourly',\n",
    "        'time_zone':       'utc+00:00',\n",
    "        'area':            AREA,\n",
    "    },\n",
    "    zip_path\n",
    ")\n",
    "\n",
    "import tempfile\n",
    "\n",
    "def ingest_from_zip(table_name, zip_filepath):\n",
    "    \"\"\"Extrae cada .nc a un NamedTemporaryFile y vuelca a SQLite.\"\"\"\n",
    "    extracted_paths = []\n",
    "\n",
    "    # 1) Abre el ZIP\n",
    "    with zipfile.ZipFile(zip_filepath, 'r') as z:\n",
    "        nc_members = [m for m in z.namelist() if m.endswith('.nc')]\n",
    "        if not nc_members:\n",
    "            raise RuntimeError(\"No encontré archivos .nc en el ZIP\")\n",
    "\n",
    "        for member in nc_members:\n",
    "            # crea un temp file garantizado escribible\n",
    "            tmpf = tempfile.NamedTemporaryFile(suffix=\".nc\", delete=False)\n",
    "            # vuelca el contenido\n",
    "            with z.open(member) as src:\n",
    "                tmpf.write(src.read())\n",
    "            tmpf.close()\n",
    "            extracted_paths.append(tmpf.name)\n",
    "\n",
    "    # 2) Lee coordenadas y variables\n",
    "    data_arrays = {}\n",
    "    time_objs = latitudes = longitudes = None\n",
    "\n",
    "    for nc_path in extracted_paths:\n",
    "        ds = Dataset(nc_path)\n",
    "        # detecta nombre de dim/vars\n",
    "        if time_objs is None:\n",
    "            time_var = next(v for v in ds.variables if \"time\" in v.lower())\n",
    "            lat_var  = next(v for v in ds.variables if \"lat\"  in v.lower())\n",
    "            lon_var  = next(v for v in ds.variables if \"lon\"  in v.lower())\n",
    "            times    = ds.variables[time_var][:]\n",
    "            time_objs= num2date(times,\n",
    "                        ds.variables[time_var].units,\n",
    "                        getattr(ds.variables[time_var], 'calendar', 'standard'))\n",
    "            latitudes = ds.variables[lat_var][:]\n",
    "            longitudes= ds.variables[lon_var][:]\n",
    "        # identifica la variable de datos\n",
    "        var_keys = [v for v in ds.variables \n",
    "                    if v not in (time_var, lat_var, lon_var, \"number\")]\n",
    "        if len(var_keys) != 1:\n",
    "            raise RuntimeError(f\"Variables inesperadas en {nc_path}: {var_keys}\")\n",
    "        var = var_keys[0]\n",
    "        data_arrays[var] = ds.variables[var][:]\n",
    "        ds.close()\n",
    "\n",
    "    # 3) Inserta en SQLite\n",
    "    sql = f\"\"\"INSERT OR REPLACE INTO {table_name}\n",
    "              (date, latitude, longitude, t2m, d2m, tp)\n",
    "              VALUES (?, ?, ?, ?, ?, ?)\"\"\"\n",
    "    cnt = 0\n",
    "    for ti, dt in enumerate(time_objs):\n",
    "        # usa strftime directamente en el objeto cftime\n",
    "        date_str = dt.strftime(\"%Y-%m-%d\")\n",
    "        for yi, lat in enumerate(latitudes):\n",
    "            for xi, lon in enumerate(longitudes):\n",
    "                v_t2m = float(data_arrays.get(\"t2m\", data_arrays.get(\"2m_temperature\"))[ti,yi,xi])\n",
    "                v_d2m = float(data_arrays.get(\"d2m\", data_arrays.get(\"2m_dewpoint_temperature\"))[ti,yi,xi])\n",
    "                v_tp  = float(data_arrays.get(\"tp\",  data_arrays.get(\"total_precipitation\"))[ti,yi,xi])\n",
    "                cur.execute(sql, (date_str, float(lat), float(lon), v_t2m, v_d2m, v_tp))\n",
    "                cnt += 1\n",
    "    conn.commit()\n",
    "\n",
    "    # 4) Borra los temporales\n",
    "    for p in extracted_paths:\n",
    "        try:\n",
    "            os.remove(p)\n",
    "        except OSError:\n",
    "            pass\n",
    "    try:\n",
    "        os.remove(zip_filepath)\n",
    "    except OSError:\n",
    "        pass\n",
    "\n",
    "    return cnt\n",
    "\n",
    "\n",
    "# ————— Ejecuta ingesta y reporta —————\n",
    "inserted = ingest_from_zip(\"climate_test\", zip_path)\n",
    "print(f\"✔ Insertadas/actualizadas {inserted} filas en climate_test\")\n",
    "\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8ac88f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# import sqlite3\n",
    "# import pandas as pd\n",
    "# import cdsapi\n",
    "# import zipfile\n",
    "# import tempfile\n",
    "# from netCDF4 import Dataset, num2date\n",
    "\n",
    "# # ————— Configuración —————\n",
    "# DB_PATH   = r\"C:\\Users\\Nainh\\Proton Drive\\nainho1306\\My files\\Licitacion\\Plataforma local\\backend\\data\\mi_base_de_datos5.db\"\n",
    "# TMP_DIR   = \"tmp_test_batches\"\n",
    "# CDS_URL   = 'https://cds.climate.copernicus.eu/api'\n",
    "# CDS_KEY   = 'd9af180c-f7f8-4a2e-8029-09018fb8c920'\n",
    "# AREA      = [-32.0, -63.0, -33.0, -61.0]\n",
    "# VARIABLES = ['2m_temperature','2m_dewpoint_temperature','total_precipitation']\n",
    "\n",
    "# os.makedirs(TMP_DIR, exist_ok=True)\n",
    "\n",
    "# # ————— Conexión y tabla —————\n",
    "# conn = sqlite3.connect(DB_PATH)\n",
    "# cur  = conn.cursor()\n",
    "# cur.execute(\"\"\"\n",
    "# CREATE TABLE IF NOT EXISTS climate_test (\n",
    "#     date TEXT,\n",
    "#     latitude REAL, longitude REAL,\n",
    "#     t2m REAL, d2m REAL, tp REAL,\n",
    "#     PRIMARY KEY(date, latitude, longitude)\n",
    "# )\n",
    "# \"\"\")\n",
    "# conn.commit()\n",
    "\n",
    "# # ————— Fechas de 2025-01-01 a ayer —————\n",
    "# start = pd.Timestamp(\"2025-01-01\")\n",
    "# end   = pd.to_datetime(\"today\").normalize() - pd.Timedelta(days=1)\n",
    "# dates = pd.date_range(start, end, freq=\"D\")\n",
    "\n",
    "# # Agrupamos por mes y ordenamos\n",
    "# periods = sorted(dates.to_series().dt.to_period('M').unique())\n",
    "# total = len(periods)\n",
    "\n",
    "# print(f\"Procesando {total} meses, de {periods[0]} a {periods[-1]}…\")\n",
    "\n",
    "# # ————— Función de ingesta —————\n",
    "# def ingest_from_zip(table_name, zip_filepath):\n",
    "#     extracted = []\n",
    "#     with zipfile.ZipFile(zip_filepath, 'r') as z:\n",
    "#         for member in z.namelist():\n",
    "#             if not member.endswith('.nc'): continue\n",
    "#             tmpf = tempfile.NamedTemporaryFile(suffix=\".nc\", delete=False)\n",
    "#             tmpf.write(z.read(member))\n",
    "#             tmpf.close()\n",
    "#             extracted.append(tmpf.name)\n",
    "\n",
    "#     data = {}; times = lats = lons = None\n",
    "#     for path in extracted:\n",
    "#         ds = Dataset(path)\n",
    "#         if times is None:\n",
    "#             tvar = next(v for v in ds.variables if \"time\" in v.lower())\n",
    "#             latv = next(v for v in ds.variables if \"lat\"  in v.lower())\n",
    "#             lonv = next(v for v in ds.variables if \"lon\"  in v.lower())\n",
    "#             times = num2date(ds.variables[tvar][:],\n",
    "#                              ds.variables[tvar].units,\n",
    "#                              getattr(ds.variables[tvar],'calendar','standard'))\n",
    "#             lats  = ds.variables[latv][:]\n",
    "#             lons  = ds.variables[lonv][:]\n",
    "#         var_keys = [v for v in ds.variables if v not in (tvar, latv, lonv, \"number\")]\n",
    "#         data[var_keys[0]] = ds.variables[var_keys[0]][:]\n",
    "#         ds.close()\n",
    "\n",
    "#     sql = f\"\"\"INSERT OR REPLACE INTO {table_name}\n",
    "#               (date, latitude, longitude, t2m, d2m, tp)\n",
    "#               VALUES (?, ?, ?, ?, ?, ?)\"\"\"\n",
    "#     cnt = 0\n",
    "#     for ti, dt in enumerate(times):\n",
    "#         dstr = dt.strftime(\"%Y-%m-%d\")\n",
    "#         for yi, lat in enumerate(lats):\n",
    "#             for xi, lon in enumerate(lons):\n",
    "#                 cur.execute(sql, (\n",
    "#                     dstr, float(lat), float(lon),\n",
    "#                     float(data.get('2m_temperature', data.get('t2m'))[ti,yi,xi]),\n",
    "#                     float(data.get('2m_dewpoint_temperature', data.get('d2m'))[ti,yi,xi]),\n",
    "#                     float(data.get('total_precipitation', data.get('tp'))[ti,yi,xi])\n",
    "#                 ))\n",
    "#                 cnt += 1\n",
    "#     conn.commit()\n",
    "#     for fpath in extracted + [zip_filepath]:\n",
    "#         try: os.remove(fpath)\n",
    "#         except: pass\n",
    "#     return cnt\n",
    "\n",
    "# # ————— Descarga e ingesta por mes con progreso —————\n",
    "# c = cdsapi.Client(url=CDS_URL, key=CDS_KEY)\n",
    "# for idx, period in enumerate(periods, start=1):\n",
    "#     year  = str(period.year)\n",
    "#     month = f\"{period.month:02d}\"\n",
    "#     # Filtrar sólo las fechas de ese mes\n",
    "#     days  = [f\"{d.day:02d}\" for d in dates if d.to_period('M') == period]\n",
    "\n",
    "#     zip_path = os.path.join(TMP_DIR, f\"{year}{month}.zip\")\n",
    "#     print(f\"[{idx}/{total}] {year}-{month}: días {days}\")\n",
    "\n",
    "#     # Descarga\n",
    "#     c.retrieve(\n",
    "#         'derived-era5-single-levels-daily-statistics',\n",
    "#         {\n",
    "#             'product_type':    'reanalysis',\n",
    "#             'format':          'netcdf',\n",
    "#             'variable':        VARIABLES,\n",
    "#             'year':            [year],\n",
    "#             'month':           [month],\n",
    "#             'day':             days,\n",
    "#             'daily_statistic': 'daily_mean',\n",
    "#             'frequency':       '6_hourly',\n",
    "#             'time_zone':       'utc+00:00',\n",
    "#             'area':            AREA,\n",
    "#         },\n",
    "#         zip_path\n",
    "#     )\n",
    "\n",
    "#     # Ingesta\n",
    "#     inserted = ingest_from_zip(\"climate_test\", zip_path)\n",
    "#     pct = idx/total*100\n",
    "#     print(f\"   ✔ Insertadas {inserted} filas. Progreso: {pct:.1f}%\")\n",
    "\n",
    "# conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "18aac9bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import winsound\n",
    "\n",
    "# Beep sencillo: frecuencia 1 kHz durante 500 ms\n",
    "winsound.Beep(1000, 1300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3de181b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          date  latitude  longitude         t2m         d2m   tp t2m_min  \\\n",
      "0   2025-07-02    -32.00     -61.00  277.063599  271.632050  0.0    None   \n",
      "1   2025-07-02    -32.00     -61.25  276.032837  270.251190  0.0    None   \n",
      "2   2025-07-02    -32.00     -61.50  275.775513  269.354218  0.0    None   \n",
      "3   2025-07-02    -32.00     -61.75  276.478638  268.784882  0.0    None   \n",
      "4   2025-07-02    -32.00     -62.00  276.754028  268.600800  0.0    None   \n",
      "5   2025-07-02    -32.00     -62.25  276.935669  268.486053  0.0    None   \n",
      "6   2025-07-02    -32.00     -62.50  277.075806  268.494354  0.0    None   \n",
      "7   2025-07-02    -32.00     -62.75  277.171997  269.055389  0.0    None   \n",
      "8   2025-07-02    -32.00     -63.00  277.255005  269.951874  0.0    None   \n",
      "9   2025-07-02    -32.25     -61.00  276.527466  271.567596  0.0    None   \n",
      "10  2025-07-02    -32.25     -61.25  275.487915  270.188202  0.0    None   \n",
      "11  2025-07-02    -32.25     -61.50  275.347778  269.501190  0.0    None   \n",
      "12  2025-07-02    -32.25     -61.75  276.085083  269.039764  0.0    None   \n",
      "13  2025-07-02    -32.25     -62.00  276.280884  268.893280  0.0    None   \n",
      "14  2025-07-02    -32.25     -62.25  276.486450  268.730194  0.0    None   \n",
      "15  2025-07-02    -32.25     -62.50  276.642212  268.734100  0.0    None   \n",
      "16  2025-07-02    -32.25     -62.75  276.720825  268.980194  0.0    None   \n",
      "17  2025-07-02    -32.25     -63.00  277.012817  269.804413  0.0    None   \n",
      "18  2025-07-02    -32.50     -61.00  275.526978  270.906464  0.0    None   \n",
      "19  2025-07-02    -32.50     -61.25  275.148071  270.090057  0.0    None   \n",
      "\n",
      "   t2m_max wind_speed  \n",
      "0     None       None  \n",
      "1     None       None  \n",
      "2     None       None  \n",
      "3     None       None  \n",
      "4     None       None  \n",
      "5     None       None  \n",
      "6     None       None  \n",
      "7     None       None  \n",
      "8     None       None  \n",
      "9     None       None  \n",
      "10    None       None  \n",
      "11    None       None  \n",
      "12    None       None  \n",
      "13    None       None  \n",
      "14    None       None  \n",
      "15    None       None  \n",
      "16    None       None  \n",
      "17    None       None  \n",
      "18    None       None  \n",
      "19    None       None  \n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "import pandas as pd\n",
    "\n",
    "# 1) Abre la conexión\n",
    "conn = sqlite3.connect(DB_PATH)\n",
    "\n",
    "# 2) Lée los últimos 20 registros (ordenados por fecha decreciente)\n",
    "df = pd.read_sql_query(\"\"\"\n",
    "    SELECT *\n",
    "      FROM climate_test\n",
    "     ORDER BY date DESC, latitude DESC, longitude DESC\n",
    "     LIMIT 20\n",
    "\"\"\", conn)\n",
    "\n",
    "# 3) Muestra el DataFrame\n",
    "print(df)\n",
    "\n",
    "# 4) Cierra la conexión\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3571eb8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-11 16:19:55,568 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Descargando datos desde 2025-07-02 hasta 2025-07-11 → 1 meses: [Period('2025-07', 'M')]\n",
      "[1/1] Descargando 2025-07: días ['02', '03', '04', '05', '06', '07', '08', '09', '10', '11'] …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-11 16:19:56,140 INFO Request ID is 632dee09-9fb6-4e70-9ce0-f96f84a479fe\n",
      "2025-07-11 16:19:56,209 INFO status has been updated to accepted\n",
      "2025-07-11 16:20:04,627 INFO status has been updated to running\n",
      "2025-07-11 16:20:17,516 INFO status has been updated to successful\n",
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   ✔ Insertadas 180 filas — Progreso: 100.0%\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import cdsapi\n",
    "import zipfile\n",
    "import tempfile\n",
    "from netCDF4 import Dataset, num2date\n",
    "\n",
    "# ————— Configuración —————\n",
    "DB_PATH   = r\"C:\\Users\\Nainh\\Proton Drive\\nainho1306\\My files\\Licitacion\\Plataforma local\\backend\\data\\mi_base_de_datos5.db\"\n",
    "TMP_DIR   = \"tmp_clima_nov_dic_2024\"\n",
    "CDS_URL   = 'https://cds.climate.copernicus.eu/api'\n",
    "CDS_KEY   = 'd9af180c-f7f8-4a2e-8029-09018fb8c920'\n",
    "AREA      = [-32.0, -63.0, -33.0, -61.0]\n",
    "VARIABLES = ['2m_temperature','2m_dewpoint_temperature','total_precipitation']\n",
    "\n",
    "os.makedirs(TMP_DIR, exist_ok=True)\n",
    "\n",
    "# ————— Conecta y asegura la tabla —————\n",
    "conn = sqlite3.connect(DB_PATH)\n",
    "cur  = conn.cursor()\n",
    "cur.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS climate_test (\n",
    "    date TEXT,\n",
    "    latitude REAL, longitude REAL,\n",
    "    t2m REAL, d2m REAL, tp REAL,\n",
    "    PRIMARY KEY(date, latitude, longitude)\n",
    ")\n",
    "\"\"\")\n",
    "conn.commit()\n",
    "\n",
    "# ————— Rango fijo de fechas —————\n",
    "start = pd.Timestamp(\"2025-07-02\")\n",
    "end   = pd.Timestamp(\"2025-07-11\")\n",
    "dates = pd.date_range(start, end, freq=\"D\")\n",
    "\n",
    "# Agrupamos y ordenamos los meses\n",
    "periods = sorted(dates.to_series().dt.to_period('M').unique())\n",
    "total = len(periods)\n",
    "print(f\"Descargando datos desde {start.date()} hasta {end.date()} → {total} meses: {periods}\")\n",
    "\n",
    "# ————— Función de ingesta —————\n",
    "def ingest_from_zip(table, zip_fp):\n",
    "    tmp_files = []\n",
    "    with zipfile.ZipFile(zip_fp) as z:\n",
    "        for member in z.namelist():\n",
    "            if not member.endswith('.nc'): continue\n",
    "            tmp = tempfile.NamedTemporaryFile(suffix=\".nc\", delete=False)\n",
    "            tmp.write(z.read(member)); tmp.close()\n",
    "            tmp_files.append(tmp.name)\n",
    "\n",
    "    data, times, lats, lons = {}, None, None, None\n",
    "    for nc in tmp_files:\n",
    "        ds = Dataset(nc)\n",
    "        if times is None:\n",
    "            tvar = next(v for v in ds.variables if \"time\" in v.lower())\n",
    "            latv = next(v for v in ds.variables if \"lat\"  in v.lower())\n",
    "            lonv = next(v for v in ds.variables if \"lon\"  in v.lower())\n",
    "            times = num2date(ds.variables[tvar][:],\n",
    "                             ds.variables[tvar].units,\n",
    "                             getattr(ds.variables[tvar],'calendar','standard'))\n",
    "            lats  = ds.variables[latv][:]\n",
    "            lons  = ds.variables[lonv][:]\n",
    "        var_keys = [v for v in ds.variables if v not in (tvar, latv, lonv, \"number\")]\n",
    "        data[var_keys[0]] = ds.variables[var_keys[0]][:]\n",
    "        ds.close()\n",
    "\n",
    "    sql = f\"\"\"INSERT OR REPLACE INTO {table}\n",
    "              (date,latitude,longitude,t2m,d2m,tp)\n",
    "              VALUES (?,?,?,?,?,?)\"\"\"\n",
    "    count = 0\n",
    "    for ti, dt in enumerate(times):\n",
    "        # Aquí usamos directamente strftime sobre el objeto cftime\n",
    "        day = dt.strftime(\"%Y-%m-%d\")\n",
    "        for yi, lat in enumerate(lats):\n",
    "            for xi, lon in enumerate(lons):\n",
    "                cur.execute(sql, (\n",
    "                    day, float(lat), float(lon),\n",
    "                    float(data.get('2m_temperature', data.get('t2m'))[ti,yi,xi]),\n",
    "                    float(data.get('2m_dewpoint_temperature', data.get('d2m'))[ti,yi,xi]),\n",
    "                    float(data.get('total_precipitation', data.get('tp'))[ti,yi,xi])\n",
    "                ))\n",
    "                count += 1\n",
    "    conn.commit()\n",
    "\n",
    "    # Limpieza\n",
    "    for f in tmp_files + [zip_fp]:\n",
    "        try: os.remove(f)\n",
    "        except: pass\n",
    "\n",
    "    return count\n",
    "\n",
    "# ————— Batch mensual con progreso —————\n",
    "client = cdsapi.Client(url=CDS_URL, key=CDS_KEY)\n",
    "for idx, period in enumerate(periods, start=1):\n",
    "    year  = str(period.year)\n",
    "    month = f\"{period.month:02d}\"\n",
    "    days  = [f\"{d.day:02d}\" for d in dates if d.to_period('M') == period]\n",
    "\n",
    "    zip_path = os.path.join(TMP_DIR, f\"{year}{month}.zip\")\n",
    "    print(f\"[{idx}/{total}] Descargando {year}-{month}: días {days} …\")\n",
    "\n",
    "    client.retrieve(\n",
    "        'derived-era5-single-levels-daily-statistics',\n",
    "        {\n",
    "            'product_type':    'reanalysis',\n",
    "            'format':          'netcdf',\n",
    "            'variable':        VARIABLES,\n",
    "            'year':            [year],\n",
    "            'month':           [month],\n",
    "            'day':             days,\n",
    "            'daily_statistic': 'daily_mean',\n",
    "            'frequency':       '6_hourly',\n",
    "            'time_zone':       'utc+00:00',\n",
    "            'area':            AREA,\n",
    "        },\n",
    "        zip_path\n",
    "    )\n",
    "\n",
    "    inserted = ingest_from_zip(\"climate_test\", zip_path)\n",
    "    pct = idx/total*100\n",
    "    print(f\"   ✔ Insertadas {inserted} filas — Progreso: {pct:.1f}%\")\n",
    "\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dffc8c6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-07 15:47:05,470 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/4] Descargando mínima 2023-12 …\n",
      "  daily_statistic: daily_min, variables: ['t2m_min']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-07 15:47:06,071 INFO Request ID is fcec75e3-f11f-464c-b8bc-6a7cbbbc67ff\n",
      "2025-07-07 15:47:06,215 INFO status has been updated to accepted\n",
      "2025-07-07 15:47:14,683 INFO status has been updated to running\n",
      "2025-07-07 15:47:19,811 INFO status has been updated to failed\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ERROR en retrieve: 400 Client Error: Bad Request for url: https://cds.climate.copernicus.eu/api/retrieve/v1/jobs/fcec75e3-f11f-464c-b8bc-6a7cbbbc67ff/results\n",
      "The job has failed.\n",
      "The job failed with: TypeError\n",
      "{\"type\":\"job results failed\",\"title\":\"The job has failed.\",\"status\":400,\"instance\":\"https://cds.climate.copernicus.eu/api/retrieve/v1/jobs/fcec75e3-f11f-464c-b8bc-6a7cbbbc67ff/results\",\"trace_id\":\"46a14de6-862c-4642-980f-ffa6f845136e\",\"traceback\":\"The job failed with: TypeError\"}\n"
     ]
    },
    {
     "ename": "HTTPError",
     "evalue": "400 Client Error: Bad Request for url: https://cds.climate.copernicus.eu/api/retrieve/v1/jobs/fcec75e3-f11f-464c-b8bc-6a7cbbbc67ff/results\nThe job has failed.\nThe job failed with: TypeError",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 140\u001b[39m\n\u001b[32m    127\u001b[39m params = {\n\u001b[32m    128\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mproduct_type\u001b[39m\u001b[33m'\u001b[39m:    \u001b[33m'\u001b[39m\u001b[33mreanalysis\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    129\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mformat\u001b[39m\u001b[33m'\u001b[39m:          \u001b[33m'\u001b[39m\u001b[33mnetcdf\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    136\u001b[39m     \u001b[33m'\u001b[39m\u001b[33marea\u001b[39m\u001b[33m'\u001b[39m:            AREA\n\u001b[32m    137\u001b[39m }\n\u001b[32m    139\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m140\u001b[39m     client.retrieve(\n\u001b[32m    141\u001b[39m         \u001b[33m'\u001b[39m\u001b[33mderived-era5-single-levels-daily-statistics\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    142\u001b[39m         params,\n\u001b[32m    143\u001b[39m         zip_fp\n\u001b[32m    144\u001b[39m     )\n\u001b[32m    145\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    146\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mERROR en retrieve:\u001b[39m\u001b[33m\"\u001b[39m, e)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nainh\\miniconda3\\envs\\licitacion\\Lib\\site-packages\\ecmwf\\datastores\\legacy_client.py:167\u001b[39m, in \u001b[36mLegacyClient.retrieve\u001b[39m\u001b[34m(self, name, request, target)\u001b[39m\n\u001b[32m    165\u001b[39m submitted: datastores.Remote | datastores.Results\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.wait_until_complete:\n\u001b[32m--> \u001b[39m\u001b[32m167\u001b[39m     submitted = \u001b[38;5;28mself\u001b[39m.client.submit_and_wait_on_results(\n\u001b[32m    168\u001b[39m         collection_id=name,\n\u001b[32m    169\u001b[39m         request=request,\n\u001b[32m    170\u001b[39m     )\n\u001b[32m    171\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    172\u001b[39m     submitted = \u001b[38;5;28mself\u001b[39m.client.submit(\n\u001b[32m    173\u001b[39m         collection_id=name,\n\u001b[32m    174\u001b[39m         request=request,\n\u001b[32m    175\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nainh\\miniconda3\\envs\\licitacion\\Lib\\site-packages\\ecmwf\\datastores\\client.py:397\u001b[39m, in \u001b[36mClient.submit_and_wait_on_results\u001b[39m\u001b[34m(self, collection_id, request)\u001b[39m\n\u001b[32m    381\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msubmit_and_wait_on_results\u001b[39m(\n\u001b[32m    382\u001b[39m     \u001b[38;5;28mself\u001b[39m, collection_id: \u001b[38;5;28mstr\u001b[39m, request: \u001b[38;5;28mdict\u001b[39m[\u001b[38;5;28mstr\u001b[39m, Any]\n\u001b[32m    383\u001b[39m ) -> datastores.Results:\n\u001b[32m    384\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Submit a request and wait for the results to be ready.\u001b[39;00m\n\u001b[32m    385\u001b[39m \n\u001b[32m    386\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    395\u001b[39m \u001b[33;03m    datastores.Results\u001b[39;00m\n\u001b[32m    396\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m397\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve_api.submit(collection_id, request).get_results()\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nainh\\miniconda3\\envs\\licitacion\\Lib\\site-packages\\ecmwf\\datastores\\processing.py:544\u001b[39m, in \u001b[36mRemote.get_results\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_results\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> Results:\n\u001b[32m    538\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Retrieve results.\u001b[39;00m\n\u001b[32m    539\u001b[39m \n\u001b[32m    540\u001b[39m \u001b[33;03m    Returns\u001b[39;00m\n\u001b[32m    541\u001b[39m \u001b[33;03m    -------\u001b[39;00m\n\u001b[32m    542\u001b[39m \u001b[33;03m    datastores.Results\u001b[39;00m\n\u001b[32m    543\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m544\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._make_results(wait=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nainh\\miniconda3\\envs\\licitacion\\Lib\\site-packages\\ecmwf\\datastores\\processing.py:519\u001b[39m, in \u001b[36mRemote._make_results\u001b[39m\u001b[34m(self, wait)\u001b[39m\n\u001b[32m    517\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_make_results\u001b[39m(\u001b[38;5;28mself\u001b[39m, wait: \u001b[38;5;28mbool\u001b[39m) -> Results:\n\u001b[32m    518\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[32m--> \u001b[39m\u001b[32m519\u001b[39m         \u001b[38;5;28mself\u001b[39m._wait_on_results()\n\u001b[32m    520\u001b[39m     response = \u001b[38;5;28mself\u001b[39m._get_api_response(\u001b[33m\"\u001b[39m\u001b[33mget\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    521\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nainh\\miniconda3\\envs\\licitacion\\Lib\\site-packages\\ecmwf\\datastores\\processing.py:497\u001b[39m, in \u001b[36mRemote._wait_on_results\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    495\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_wait_on_results\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    496\u001b[39m     sleep = \u001b[32m1.0\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m497\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.results_ready:\n\u001b[32m    498\u001b[39m         \u001b[38;5;28mself\u001b[39m.debug(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mresults not ready, waiting for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msleep\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m seconds\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    499\u001b[39m         time.sleep(sleep)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nainh\\miniconda3\\envs\\licitacion\\Lib\\site-packages\\ecmwf\\datastores\\processing.py:511\u001b[39m, in \u001b[36mRemote.results_ready\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    509\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[32m    510\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m status == \u001b[33m\"\u001b[39m\u001b[33mfailed\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m--> \u001b[39m\u001b[32m511\u001b[39m     results = \u001b[38;5;28mself\u001b[39m._make_results(wait=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m    512\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m ProcessingFailedError(error_json_to_message(results._json_dict))\n\u001b[32m    513\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m status \u001b[38;5;129;01min\u001b[39;00m (\u001b[33m\"\u001b[39m\u001b[33mdismissed\u001b[39m\u001b[33m\"\u001b[39m, \u001b[33m\"\u001b[39m\u001b[33mdeleted\u001b[39m\u001b[33m\"\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nainh\\miniconda3\\envs\\licitacion\\Lib\\site-packages\\ecmwf\\datastores\\processing.py:525\u001b[39m, in \u001b[36mRemote._make_results\u001b[39m\u001b[34m(self, wait)\u001b[39m\n\u001b[32m    523\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m LinkError:\n\u001b[32m    524\u001b[39m     results_url = \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/results\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m525\u001b[39m results = Results.from_request(\u001b[33m\"\u001b[39m\u001b[33mget\u001b[39m\u001b[33m\"\u001b[39m, results_url, **\u001b[38;5;28mself\u001b[39m._request_kwargs)\n\u001b[32m    526\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m results\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nainh\\miniconda3\\envs\\licitacion\\Lib\\site-packages\\ecmwf\\datastores\\processing.py:176\u001b[39m, in \u001b[36mApiResponse.from_request\u001b[39m\u001b[34m(cls, method, url, headers, session, retry_options, request_options, download_options, sleep_max, cleanup, log_callback, log_messages, **kwargs)\u001b[39m\n\u001b[32m    171\u001b[39m response = robust_request(\n\u001b[32m    172\u001b[39m     method, url, headers=headers, **request_options, **kwargs\n\u001b[32m    173\u001b[39m )\n\u001b[32m    174\u001b[39m log(logging.DEBUG, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mREPLY \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.text\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m, callback=log_callback)\n\u001b[32m--> \u001b[39m\u001b[32m176\u001b[39m cads_raise_for_status(response)\n\u001b[32m    178\u001b[39m \u001b[38;5;28mself\u001b[39m = \u001b[38;5;28mcls\u001b[39m(\n\u001b[32m    179\u001b[39m     response,\n\u001b[32m    180\u001b[39m     headers=headers,\n\u001b[32m   (...)\u001b[39m\u001b[32m    187\u001b[39m     log_callback=log_callback,\n\u001b[32m    188\u001b[39m )\n\u001b[32m    189\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m log_messages:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nainh\\miniconda3\\envs\\licitacion\\Lib\\site-packages\\ecmwf\\datastores\\processing.py:99\u001b[39m, in \u001b[36mcads_raise_for_status\u001b[39m\u001b[34m(response)\u001b[39m\n\u001b[32m     92\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m     93\u001b[39m         message = \u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m.join(\n\u001b[32m     94\u001b[39m             [\n\u001b[32m     95\u001b[39m                 \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.status_code\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m Client Error: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.reason\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m for url: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresponse.url\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m,\n\u001b[32m     96\u001b[39m                 error_json_to_message(error_json),\n\u001b[32m     97\u001b[39m             ]\n\u001b[32m     98\u001b[39m         )\n\u001b[32m---> \u001b[39m\u001b[32m99\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m requests.HTTPError(message, response=response)\n\u001b[32m    100\u001b[39m response.raise_for_status()\n",
      "\u001b[31mHTTPError\u001b[39m: 400 Client Error: Bad Request for url: https://cds.climate.copernicus.eu/api/retrieve/v1/jobs/fcec75e3-f11f-464c-b8bc-6a7cbbbc67ff/results\nThe job has failed.\nThe job failed with: TypeError"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import cdsapi\n",
    "import zipfile\n",
    "import tempfile\n",
    "from netCDF4 import Dataset, num2date\n",
    "\n",
    "# ————— Configuración —————\n",
    "DB_PATH   = r\"C:\\Users\\Nainh\\Proton Drive\\nainho1306\\My files\\Licitacion\\Plataforma local\\backend\\data\\mi_base_de_datos5.db\"\n",
    "TMP_DIR   = \"tmp_test20d\"\n",
    "CDS_URL   = 'https://cds.climate.copernicus.eu/api'\n",
    "CDS_KEY   = 'd9af180c-f7f8-4a2e-8029-09018fb8c920'\n",
    "AREA      = [-32.0, -63.0, -33.0, -61.0]  # [Norte, Oeste, Sur, Este]\n",
    "VARIABLES = ['2m_temperature','2m_dewpoint_temperature','total_precipitation']\n",
    "\n",
    "# Variables y columnas destino por estadística\n",
    "STATS = {\n",
    "    'mínima':  ('daily_min', ['t2m_min']),\n",
    "    'máxima':  ('daily_max', ['t2m_max']),\n",
    "    'viento':  ('daily_mean', ['10m_u_component_of_wind',\n",
    "                               '10m_v_component_of_wind'])\n",
    "}\n",
    "\n",
    "os.makedirs(TMP_DIR, exist_ok=True)\n",
    "conn = sqlite3.connect(DB_PATH)\n",
    "cur  = conn.cursor()\n",
    "\n",
    "# Asegúrate de tablas y columnas\n",
    "cur.execute(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS climate_stats (\n",
    "    date TEXT, latitude REAL, longitude REAL,\n",
    "    t2m_min REAL, t2m_max REAL,\n",
    "    u10_mean REAL, v10_mean REAL, wind_speed REAL,\n",
    "    PRIMARY KEY(date, latitude, longitude)\n",
    ")\"\"\")\n",
    "conn.commit()\n",
    "\n",
    "def ingest_zip(table, file_fp, vars_nc, cols):\n",
    "    \"\"\"Extrae .nc del ZIP, lee con netCDF4 y vuelca cols en la tabla.\"\"\"\n",
    "    # 1) Extraer .nc(s)\n",
    "    paths = []\n",
    "    if zipfile.is_zipfile(file_fp):\n",
    "        with zipfile.ZipFile(file_fp, 'r') as z:\n",
    "            members = [m for m in z.namelist() if m.endswith('.nc')]\n",
    "            if not members:\n",
    "                raise RuntimeError(f\"ZIP {file_fp} no contiene .nc\")\n",
    "            for m in members:\n",
    "                tmp = tempfile.NamedTemporaryFile(suffix=\".nc\", delete=False)\n",
    "                tmp.write(z.read(m))\n",
    "                tmp.close()\n",
    "                paths.append(tmp.name)\n",
    "    else:\n",
    "        # 2) No es ZIP: leemos magic-bytes para ver si HDF5/netCDF3\n",
    "        with open(file_fp, 'rb') as f:\n",
    "            hb = f.read(8)\n",
    "        if hb.startswith(b'\\211HDF') or b'CDF' in hb:\n",
    "            paths = [file_fp]\n",
    "        else:\n",
    "            raise RuntimeError(\n",
    "                f\"{file_fp} no es ZIP ni NetCDF válido, primeros bytes: {hb!r}\"\n",
    "            )\n",
    "\n",
    "    # 3) Leer coords/times y cada variable\n",
    "    data = {}\n",
    "    times = lats = lons = None\n",
    "    for p in paths:\n",
    "        ds = Dataset(p)\n",
    "        if times is None:\n",
    "            tvar = next(v for v in ds.variables if \"time\" in v.lower())\n",
    "            latv = next(v for v in ds.variables if \"lat\"  in v.lower())\n",
    "            lonv = next(v for v in ds.variables if \"lon\"  in v.lower())\n",
    "            times = num2date(ds.variables[tvar][:],\n",
    "                             ds.variables[tvar].units,\n",
    "                             getattr(ds.variables[tvar],'calendar','standard'))\n",
    "            lats = ds.variables[latv][:]\n",
    "            lons = ds.variables[lonv][:]\n",
    "        for var, col in zip(vars_nc, cols):\n",
    "            data[col] = ds.variables[var][:]\n",
    "        ds.close()\n",
    "\n",
    "    # 4) Insert/upsert\n",
    "    cols_sql = \",\".join(cols)\n",
    "    vals_sql = \",\".join(\"?\" for _ in cols)\n",
    "    sql = f\"\"\"\n",
    "      INSERT INTO {table}\n",
    "        (date, latitude, longitude, {cols_sql})\n",
    "      VALUES (?,?,?,{vals_sql})\n",
    "      ON CONFLICT(date,latitude,longitude) DO UPDATE SET\n",
    "        {\", \".join(f\"{c}=excluded.{c}\" for c in cols)}\n",
    "    \"\"\"\n",
    "    for ti, dt in enumerate(times):\n",
    "        day = dt.strftime(\"%Y-%m-%d\")\n",
    "        for yi, lat in enumerate(lats):\n",
    "            for xi, lon in enumerate(lons):\n",
    "                row = [day, float(lat), float(lon)] + [float(data[c][ti,yi,xi]) for c in cols]\n",
    "                cur.execute(sql, row)\n",
    "    conn.commit()\n",
    "\n",
    "    # 5) limpiar temporales\n",
    "    for p in paths + [file_fp]:\n",
    "        try:\n",
    "            os.remove(p)\n",
    "        except OSError:\n",
    "            pass\n",
    "\n",
    "# ————— Bucle principal —————\n",
    "client = cdsapi.Client(url=CDS_URL, key=CDS_KEY)\n",
    "\n",
    "# Rango de ejemplo: diciembre 2023–marzo 2024\n",
    "start = pd.Timestamp(\"2023-12-01\")\n",
    "end   = pd.Timestamp(\"2024-03-31\")\n",
    "dates = pd.date_range(start, end, freq=\"D\")\n",
    "periods = sorted(dates.to_series().dt.to_period('M').unique())\n",
    "total = len(periods)\n",
    "\n",
    "for stat_name, (daily_stat, cols) in STATS.items():\n",
    "    for idx, period in enumerate(periods, start=1):\n",
    "        y = str(period.year)\n",
    "        m = f\"{period.month:02d}\"\n",
    "        days = [f\"{d.day:02d}\" for d in dates if d.to_period('M') == period]\n",
    "        zip_fp = os.path.join(TMP_DIR, f\"{y}{m}_{stat_name}.zip\")\n",
    "\n",
    "        print(f\"[{idx}/{total}] Descargando {stat_name} {y}-{m} …\")\n",
    "        print(f\"  daily_statistic: {daily_stat}, variables: {cols}\")\n",
    "\n",
    "        params = {\n",
    "            'product_type':    'reanalysis',\n",
    "            'format':          'netcdf',\n",
    "            'variable':        cols if stat_name == 'viento' else ['2m_temperature'],\n",
    "            'year':            [y],\n",
    "            'month':           [m],\n",
    "            'day':             days,\n",
    "            'daily_statistic': [daily_stat],  # ahora en lista\n",
    "            'frequency':       'daily',\n",
    "            'area':            AREA\n",
    "        }\n",
    "\n",
    "        try:\n",
    "            client.retrieve(\n",
    "                'derived-era5-single-levels-daily-statistics',\n",
    "                params,\n",
    "                zip_fp\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(\"ERROR en retrieve:\", e)\n",
    "            # Si CDS devuelve cuerpo de error JSON, lo mostramos:\n",
    "            if hasattr(e, 'response') and e.response is not None:\n",
    "                try:\n",
    "                    print(e.response.text)\n",
    "                except Exception:\n",
    "                    pass\n",
    "            raise\n",
    "\n",
    "        ingest_zip(\n",
    "            \"climate_stats\",\n",
    "            zip_fp,\n",
    "            cols if stat_name != 'viento' else\n",
    "                ['10m_u_component_of_wind', '10m_v_component_of_wind'],\n",
    "            cols\n",
    "        )\n",
    "        print(f\"   ✔ {stat_name} {y}-{m} insertado\")\n",
    "\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "984aaa16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-26 13:40:16,384 WARNING [2025-06-23T00:00:00] Scheduled System Session affecting Service reliability - 30 June 2025. Please follow status [here](https://status.ecmwf.int/) or in our [forum](https://forum.ecmwf.int/t/scheduled-maintenance-of-the-cloud-infrastructure-on-30-june-2025/13598)\n",
      "2025-06-26 13:40:16,385 INFO [2025-06-16T00:00:00] CC-BY licence to replace Licence to use Copernicus Products on 02 July 2025. More information available [here](https://forum.ecmwf.int/t/cc-by-licence-to-replace-licence-to-use-copernicus-products-on-02-july-2025/13464)\n",
      "2025-06-26 13:40:16,386 INFO [2025-06-10T00:00:00] To improve our C3S service, we need to hear from you! Please complete this very short [survey](https://confluence.ecmwf.int/x/E7uBEQ/). Thank you.\n",
      "2025-06-26 13:40:16,387 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2025-06-26 13:40:17,079 INFO Request ID is cdd49973-ffec-4335-bc46-42bedcf6cdb4\n",
      "2025-06-26 13:40:17,169 INFO status has been updated to accepted\n",
      "2025-06-26 13:40:30,907 INFO status has been updated to running\n",
      "2025-06-26 13:40:38,613 INFO status has been updated to successful\n"
     ]
    },
    {
     "ename": "PermissionError",
     "evalue": "[WinError 32] El proceso no tiene acceso al archivo porque está siendo utilizado por otro proceso: 'tmp_pts\\\\202503_-31.75_-64.5.nc'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPermissionError\u001b[39m                           Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 41\u001b[39m\n\u001b[32m     39\u001b[39m out  = os.path.join(TMP, \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00myyyy\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mmm\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlat\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m_\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlon\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.nc\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     40\u001b[39m \u001b[38;5;66;03m# 2) descarga puntual\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m41\u001b[39m c.retrieve(\n\u001b[32m     42\u001b[39m   \u001b[33m\"\u001b[39m\u001b[33mderived-era5-single-levels-daily-statistics\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     43\u001b[39m   {\n\u001b[32m     44\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mproduct_type\u001b[39m\u001b[33m\"\u001b[39m:    \u001b[33m\"\u001b[39m\u001b[33mreanalysis\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     45\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mformat\u001b[39m\u001b[33m\"\u001b[39m:          \u001b[33m\"\u001b[39m\u001b[33mnetcdf\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     46\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mvariable\u001b[39m\u001b[33m\"\u001b[39m:        [\u001b[33m\"\u001b[39m\u001b[33m2m_temperature\u001b[39m\u001b[33m\"\u001b[39m],\n\u001b[32m     47\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33myear\u001b[39m\u001b[33m\"\u001b[39m:            [yyyy],\n\u001b[32m     48\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mmonth\u001b[39m\u001b[33m\"\u001b[39m:           [mm],\n\u001b[32m     49\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mday\u001b[39m\u001b[33m\"\u001b[39m:             days,\n\u001b[32m     50\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mdaily_statistic\u001b[39m\u001b[33m\"\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mdaily_mean\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     51\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mfrequency\u001b[39m\u001b[33m\"\u001b[39m:       \u001b[33m\"\u001b[39m\u001b[33m1_hourly\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     52\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mtime_zone\u001b[39m\u001b[33m\"\u001b[39m:       \u001b[33m\"\u001b[39m\u001b[33mutc+00:00\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     53\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33marea\u001b[39m\u001b[33m\"\u001b[39m:            area,\n\u001b[32m     54\u001b[39m   },\n\u001b[32m     55\u001b[39m   out\n\u001b[32m     56\u001b[39m )\n\u001b[32m     57\u001b[39m \u001b[38;5;66;03m# 3) asigna valores al df\u001b[39;00m\n\u001b[32m     58\u001b[39m ds = xr.open_dataset(out)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nainh\\miniconda3\\envs\\licitacion\\Lib\\site-packages\\ecmwf\\datastores\\legacy_client.py:177\u001b[39m, in \u001b[36mLegacyClient.retrieve\u001b[39m\u001b[34m(self, name, request, target)\u001b[39m\n\u001b[32m    171\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    172\u001b[39m     submitted = \u001b[38;5;28mself\u001b[39m.client.submit(\n\u001b[32m    173\u001b[39m         collection_id=name,\n\u001b[32m    174\u001b[39m         request=request,\n\u001b[32m    175\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m177\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m submitted \u001b[38;5;28;01mif\u001b[39;00m target \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m submitted.download(target)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Nainh\\miniconda3\\envs\\licitacion\\Lib\\site-packages\\ecmwf\\datastores\\processing.py:722\u001b[39m, in \u001b[36mResults.download\u001b[39m\u001b[34m(self, target)\u001b[39m\n\u001b[32m    719\u001b[39m     target = parts.path.strip(\u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m).split(\u001b[33m\"\u001b[39m\u001b[33m/\u001b[39m\u001b[33m\"\u001b[39m)[-\u001b[32m1\u001b[39m]\n\u001b[32m    721\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m os.path.exists(target):\n\u001b[32m--> \u001b[39m\u001b[32m722\u001b[39m     os.remove(target)\n\u001b[32m    724\u001b[39m robust_download = multiurl.robust(\u001b[38;5;28mself\u001b[39m._download, **\u001b[38;5;28mself\u001b[39m.retry_options)\n\u001b[32m    725\u001b[39m robust_download(url, target)\n",
      "\u001b[31mPermissionError\u001b[39m: [WinError 32] El proceso no tiene acceso al archivo porque está siendo utilizado por otro proceso: 'tmp_pts\\\\202503_-31.75_-64.5.nc'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import cdsapi, xarray as xr, os\n",
    "\n",
    "# tu DataFrame original\n",
    "df = pd.read_csv(\"puntos.csv\", parse_dates=[\"Fechas notificados\"])\n",
    "df[\"lat\"] = df[\"lat\"].astype(float)\n",
    "df[\"lon\"] = df[\"lon\"].astype(float)\n",
    "\n",
    "# configura CDS\n",
    "\n",
    "\n",
    "CDS_URL   = 'https://cds.climate.copernicus.eu/api'\n",
    "CDS_KEY   = 'd9af180c-f7f8-4a2e-8029-09018fb8c920'\n",
    "\n",
    "c = cdsapi.Client(url=CDS_URL, key=CDS_KEY)\n",
    "\n",
    "\n",
    "\n",
    "TMP = \"tmp_pts\"\n",
    "os.makedirs(TMP, exist_ok=True)\n",
    "\n",
    "# constante semicírculo de celda ERA5\n",
    "HALF = 0.125\n",
    "\n",
    "# 1) recorre cada par único (lon,lat)\n",
    "for (lon, lat), sub in df.groupby([\"lon\",\"lat\"]):\n",
    "    # agrupa por mes\n",
    "    sub[\"mes\"] = sub[\"Fechas notificados\"].dt.to_period(\"M\")\n",
    "    for period, chunk in sub.groupby(\"mes\"):\n",
    "        yyyy = str(period.year)\n",
    "        mm   = f\"{period.month:02d}\"\n",
    "        days = sorted(chunk[\"Fechas notificados\"]\n",
    "                      .dt.day\n",
    "                      .astype(str)\n",
    "                      .str.zfill(2)\n",
    "                      .unique().tolist())\n",
    "        # define área 1×1\n",
    "        area = [lat+HALF, lon-HALF, lat-HALF, lon+HALF]\n",
    "        out  = os.path.join(TMP, f\"{yyyy}{mm}_{lat}_{lon}.nc\")\n",
    "        # 2) descarga puntual\n",
    "        c.retrieve(\n",
    "          \"derived-era5-single-levels-daily-statistics\",\n",
    "          {\n",
    "            \"product_type\":    \"reanalysis\",\n",
    "            \"format\":          \"netcdf\",\n",
    "            \"variable\":        [\"2m_temperature\"],\n",
    "            \"year\":            [yyyy],\n",
    "            \"month\":           [mm],\n",
    "            \"day\":             days,\n",
    "            \"daily_statistic\": \"daily_mean\",\n",
    "            \"frequency\":       \"1_hourly\",\n",
    "            \"time_zone\":       \"utc+00:00\",\n",
    "            \"area\":            area,\n",
    "          },\n",
    "          out\n",
    "        )\n",
    "        # 3) asigna valores al df\n",
    "        ds = xr.open_dataset(out)\n",
    "        for _, row in chunk.iterrows():\n",
    "            t = row[\"Fechas notificados\"].strftime(\"%Y-%m-%d\")\n",
    "            val = ds[\"t2m\"].sel(\n",
    "                    latitude  = lat,\n",
    "                    longitude = lon,\n",
    "                    time      = t,\n",
    "                    method    = \"nearest\"\n",
    "                  ).item()\n",
    "            df.loc[row.name, \"t2m\"] = val\n",
    "        ds.close()\n",
    "        os.remove(out)\n",
    "\n",
    "print(df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017b4ff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-25 16:05:37,750 WARNING [2025-06-23T00:00:00] Scheduled System Session affecting Service reliability - 30 June 2025. Please follow status [here](https://status.ecmwf.int/) or in our [forum](https://forum.ecmwf.int/t/scheduled-maintenance-of-the-cloud-infrastructure-on-30-june-2025/13598)\n",
      "2025-06-25 16:05:37,751 INFO [2025-06-16T00:00:00] CC-BY licence to replace Licence to use Copernicus Products on 02 July 2025. More information available [here](https://forum.ecmwf.int/t/cc-by-licence-to-replace-licence-to-use-copernicus-products-on-02-july-2025/13464)\n",
      "2025-06-25 16:05:37,751 INFO [2025-06-10T00:00:00] To improve our C3S service, we need to hear from you! Please complete this very short [survey](https://confluence.ecmwf.int/x/E7uBEQ/). Thank you.\n",
      "2025-06-25 16:05:37,752 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n",
      "2025-06-25 16:05:39,694 INFO Request ID is afb3b3ff-1b48-44e9-b3c6-cf8ed05b7912\n",
      "2025-06-25 16:05:39,804 INFO status has been updated to accepted\n",
      "2025-06-25 16:05:53,674 INFO status has been updated to successful\n",
      "                                                                                         \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'tmp_test20d\\\\test20d.zip'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import cdsapi\n",
    "c = cdsapi.Client(url=CDS_URL, key=CDS_KEY)\n",
    "dataset = \"derived-era5-single-levels-daily-statistics\"\n",
    "request = {\n",
    "    \"product_type\": \"reanalysis\",\n",
    "    \"variable\": [\"2m_temperature\"],\n",
    "    \"year\": \"2023\",\n",
    "    \"month\": [\"12\"],\n",
    "    \"day\": [\"02\", \"03\", \"04\"],\n",
    "    \"daily_statistic\": \"daily_minimum\",\n",
    "    \"time_zone\": \"utc+00:00\",\n",
    "    \"frequency\": \"1_hourly\",\n",
    "    \"area\": [-32, -63, -33, -61]\n",
    "}\n",
    "\n",
    "\n",
    "c.retrieve(\n",
    "  'derived-era5-single-levels-daily-statistics',\n",
    "  {\n",
    "    'product_type':    'reanalysis',\n",
    "    'format':          'netcdf',\n",
    "    'variable':        ['2m_temperature'],\n",
    "    'year':            ['2025'],\n",
    "    'month':           ['02'],\n",
    "    'day':             ['01','02','03'],\n",
    "    'daily_statistic': 'daily_min',     # mínima diaria\n",
    "    'frequency':       '1_hourly',      # muestreo cada hora\n",
    "    'time_zone':       'utc+00:00',\n",
    "    'area':            AREA,\n",
    "  },\n",
    "  zip_path\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "761bbc66",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'valid_time': \"<class 'netCDF4.Dimension'>\": name = 'valid_time', size = 3, 'latitude': \"<class 'netCDF4.Dimension'>\": name = 'latitude', size = 5, 'longitude': \"<class 'netCDF4.Dimension'>\": name = 'longitude', size = 9}\n",
      "dict_keys(['t2m', 'number', 'latitude', 'longitude', 'valid_time'])\n",
      "<class 'netCDF4.Dataset'>\n",
      "root group (NETCDF4 data model, file format HDF5):\n",
      "    GRIB_centre: ecmf\n",
      "    GRIB_centreDescription: European Centre for Medium-Range Weather Forecasts\n",
      "    GRIB_subCentre: 0\n",
      "    Conventions: CF-1.7\n",
      "    institution: European Centre for Medium-Range Weather Forecasts\n",
      "    history: 2025-06-25T13:57 GRIB to CDM+CF via cfgrib-0.9.15.0/ecCodes-2.41.0 with {\"source\": \"2m_temperature.grib\", \"filter_by_keys\": {\"stream\": [\"oper\"]}, \"encode_cf\": [\"parameter\", \"time\", \"geography\", \"vertical\"]}\n",
      "earthkit.transforms.aggregate.temporal.daily_reduce(2m_temperature_stream-oper, how=min, **{'time_shift': {'hours': 0}})\n",
      "    dimensions(sizes): valid_time(3), latitude(5), longitude(9)\n",
      "    variables(dimensions): float32 t2m(valid_time, latitude, longitude), int64 number(), float64 latitude(latitude), float64 longitude(longitude), int64 valid_time(valid_time)\n",
      "    groups: \n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MaskedArray' object has no attribute 'units'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[21]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;66;03m# Metadatos generales\u001b[39;00m\n\u001b[32m     15\u001b[39m \u001b[38;5;28mprint\u001b[39m(ds)          \u001b[38;5;66;03m# cabeceras, atributos globales\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[38;5;28mprint\u001b[39m(t2m.units)   \u001b[38;5;66;03m# atributos de la variable\u001b[39;00m\n\u001b[32m     18\u001b[39m ds.close()\n",
      "\u001b[31mAttributeError\u001b[39m: 'MaskedArray' object has no attribute 'units'"
     ]
    }
   ],
   "source": [
    "from netCDF4 import Dataset\n",
    "\n",
    "# Abre el archivo (modo lectura por defecto)\n",
    "ds = Dataset(\"out.nc\", \"r\")\n",
    "\n",
    "# Explora sus dimensiones\n",
    "print(ds.dimensions)        \n",
    "# Explora sus variables\n",
    "print(ds.variables.keys())  \n",
    "\n",
    "# Accede a una variable concreta, por ejemplo “t2m”\n",
    "t2m = ds.variables[\"t2m\"][:]   # carga todos sus datos en memoria\n",
    "\n",
    "# Metadatos generales\n",
    "print(ds)          # cabeceras, atributos globales\n",
    "print(t2m.units)   # atributos de la variable\n",
    "\n",
    "ds.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f0853f3c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-11 16:13:53,064 INFO [2024-09-26T00:00:00] Watch our [Forum](https://forum.ecmwf.int/) for Announcements, news and other discussed topics.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Procesando 2025-07, días: ['02', '03', '04', '05', '06', '07', '08', '09', '10']\n",
      "  ▷ Descargando t2m_min… "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-11 16:13:53,924 INFO Request ID is fd4e501f-f66a-415c-9c0f-025c77e39c8d\n",
      "2025-07-11 16:13:53,989 INFO status has been updated to accepted\n",
      "2025-07-11 16:14:02,595 INFO status has been updated to running\n",
      "2025-07-11 16:14:07,730 INFO status has been updated to accepted\n",
      "2025-07-11 16:14:15,397 INFO status has been updated to successful\n",
      "                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "  ▷ Descargando t2m_max… "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-11 16:14:16,720 INFO Request ID is 7807ae49-8800-497f-81b2-d96c9d5a84be\n",
      "2025-07-11 16:14:16,775 INFO status has been updated to accepted\n",
      "2025-07-11 16:14:30,272 INFO status has been updated to running\n",
      "2025-07-11 16:14:37,946 INFO status has been updated to successful\n",
      "                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "  ▷ Descargando d2m_mean… "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-11 16:14:39,166 INFO Request ID is d8d7a176-3b08-4c85-b413-f7754c4234ef\n",
      "2025-07-11 16:14:39,245 INFO status has been updated to accepted\n",
      "2025-07-11 16:15:00,480 INFO status has been updated to running\n",
      "2025-07-11 16:15:11,930 INFO status has been updated to successful\n",
      "                                                                                        "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "  ▷ Descargando tp_sum… "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-11 16:15:13,088 INFO Request ID is 9124e726-b1f8-4c58-a1a8-1f666cbcc30a\n",
      "2025-07-11 16:15:13,162 INFO status has been updated to accepted\n",
      "2025-07-11 16:15:21,542 INFO status has been updated to running\n",
      "2025-07-11 16:15:34,329 INFO status has been updated to successful\n",
      "                                                                                        \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK\n",
      "  ▷ Guardando combinado → tmp_nc_batches\\202507_daily_stats.nc\n",
      "  ▷ Insertando en SQLite desde tmp_nc_batches\\202507_daily_stats.nc\n"
     ]
    },
    {
     "ename": "OperationalError",
     "evalue": "table climate_stats has no column named min",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOperationalError\u001b[39m                          Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 144\u001b[39m\n\u001b[32m    142\u001b[39m \u001b[38;5;66;03m# 4) Insertar en SQLite\u001b[39;00m\n\u001b[32m    143\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m  ▷ Insertando en SQLite desde \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_fp\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m144\u001b[39m insert_nc_to_sql(out_fp, DB_PATH, table=\u001b[33m\"\u001b[39m\u001b[33mclimate_stats\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    146\u001b[39m \u001b[38;5;66;03m# 5) Limpiar temporales individuales\u001b[39;00m\n\u001b[32m    147\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m fp \u001b[38;5;129;01min\u001b[39;00m tmp_files:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 84\u001b[39m, in \u001b[36minsert_nc_to_sql\u001b[39m\u001b[34m(nc_path, db_path, table)\u001b[39m\n\u001b[32m     81\u001b[39m         \u001b[38;5;28;01mfor\u001b[39;00m xi, lon \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(lons):\n\u001b[32m     82\u001b[39m             row = [date_str, \u001b[38;5;28mfloat\u001b[39m(lat), \u001b[38;5;28mfloat\u001b[39m(lon)] + \\\n\u001b[32m     83\u001b[39m                   [\u001b[38;5;28mfloat\u001b[39m(ds[v].values[ti, yi, xi]) \u001b[38;5;28;01mfor\u001b[39;00m v \u001b[38;5;129;01min\u001b[39;00m vars_sql]\n\u001b[32m---> \u001b[39m\u001b[32m84\u001b[39m             cur.execute(sql, row)\n\u001b[32m     86\u001b[39m conn.commit()\n\u001b[32m     87\u001b[39m conn.close()\n",
      "\u001b[31mOperationalError\u001b[39m: table climate_stats has no column named min"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import cdsapi\n",
    "import xarray as xr\n",
    "import sqlite3\n",
    "\n",
    "# ————— Configuración —————\n",
    "DB_PATH = r\"C:\\Users\\Nainh\\Proton Drive\\nainho1306\\My files\\Licitacion\\Plataforma local\\backend\\data\\mi_base_de_datos5.db\"\n",
    "TMP_DIR = \"tmp_nc_batches\"\n",
    "CDS_URL = 'https://cds.climate.copernicus.eu/api'\n",
    "CDS_KEY = 'd9af180c-f7f8-4a2e-8029-09018fb8c920'\n",
    "AREA    = [-32.0, -63.0, -33.0, -61.0]  # [Norte, Oeste, Sur, Este]\n",
    "\n",
    "os.makedirs(TMP_DIR, exist_ok=True)\n",
    "\n",
    "# Estadísticas a descargar: (sufijo en el NC, daily_statistic, variable API, frecuencia)\n",
    "STATS = [\n",
    "    (\"t2m_min\",  \"daily_min\",  \"2m_temperature\",         \"1_hourly\"),\n",
    "    (\"t2m_max\",  \"daily_max\",  \"2m_temperature\",         \"1_hourly\"),\n",
    "    (\"d2m_mean\", \"daily_mean\", \"2m_dewpoint_temperature\",\"6_hourly\"),\n",
    "    (\"tp_sum\",   \"daily_sum\",  \"total_precipitation\",    \"6_hourly\"),\n",
    "]\n",
    "\n",
    "# Rango: desde 2025-01-01 hasta ayer\n",
    "start   = pd.Timestamp(\"2025-07-02\")\n",
    "end     = pd.Timestamp(\"today\").normalize() - pd.Timedelta(days=1)\n",
    "dates   = pd.date_range(start, end, freq=\"D\")\n",
    "periods = sorted(dates.to_series().dt.to_period(\"M\").unique())\n",
    "\n",
    "client = cdsapi.Client(url=CDS_URL, key=CDS_KEY)\n",
    "\n",
    "def insert_nc_to_sql(nc_path, db_path, table=\"climate_stats\"):\n",
    "    \"\"\"\n",
    "    Inserta o actualiza en SQLite los datos del NetCDF que tenga variables\n",
    "    diarias combinadas (t2m_min, t2m_max, d2m_mean, tp_sum, etc.).\n",
    "    Detecta automáticamente el nombre de la dimensión tiempo.\n",
    "    \"\"\"\n",
    "    ds = xr.open_dataset(nc_path, engine=\"netcdf4\")\n",
    "    vars_sql = list(ds.data_vars)  # ej: ['t2m_min','t2m_max','d2m_mean','tp_sum']\n",
    "\n",
    "    # Detectar ejes: tomamos las dims de la primera variable\n",
    "    sample = vars_sql[0]\n",
    "    dims = ds[sample].dims\n",
    "    time_dim = next(d for d in dims if d.lower().endswith(\"time\"))\n",
    "    lat_dim  = next(d for d in dims if \"lat\"  in d.lower())\n",
    "    lon_dim  = next(d for d in dims if \"lon\"  in d.lower())\n",
    "\n",
    "    times = pd.to_datetime(ds[time_dim].values)\n",
    "    lats  = ds[lat_dim].values\n",
    "    lons  = ds[lon_dim].values\n",
    "\n",
    "    # Conectar y preparar tabla\n",
    "    conn = sqlite3.connect(db_path)\n",
    "    cur  = conn.cursor()\n",
    "    cols_def = \", \".join(f\"{v} REAL\" for v in vars_sql)\n",
    "    cur.execute(f\"\"\"\n",
    "      CREATE TABLE IF NOT EXISTS {table} (\n",
    "        date TEXT,\n",
    "        latitude REAL, longitude REAL,\n",
    "        {cols_def},\n",
    "        PRIMARY KEY(date, latitude, longitude)\n",
    "      )\n",
    "    \"\"\")\n",
    "    conn.commit()\n",
    "\n",
    "    # SQL dinámico: INSERT ... ON CONFLICT DO UPDATE\n",
    "    cols_sql = \",\".join(vars_sql)\n",
    "    ph       = \",\".join(\"?\" for _ in vars_sql)\n",
    "    sql = f\"\"\"\n",
    "      INSERT INTO {table}\n",
    "        (date, latitude, longitude, {cols_sql})\n",
    "      VALUES (?,?,?,{ph})\n",
    "      ON CONFLICT(date,latitude,longitude) DO UPDATE SET\n",
    "        {\", \".join(f\"{v}=excluded.{v}\" for v in vars_sql)}\n",
    "    \"\"\"\n",
    "\n",
    "    # Volcar fila a fila\n",
    "    for ti, dt in enumerate(times):\n",
    "        date_str = dt.strftime(\"%Y-%m-%d\")\n",
    "        for yi, lat in enumerate(lats):\n",
    "            for xi, lon in enumerate(lons):\n",
    "                row = [date_str, float(lat), float(lon)] + \\\n",
    "                      [float(ds[v].values[ti, yi, xi]) for v in vars_sql]\n",
    "                cur.execute(sql, row)\n",
    "\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    ds.close()\n",
    "\n",
    "# ————— Bucle principal —————\n",
    "for period in periods:\n",
    "    y, m = str(period.year), f\"{period.month:02d}\"\n",
    "    days = [f\"{d.day:02d}\" for d in dates if d.to_period(\"M\") == period]\n",
    "    print(f\"\\nProcesando {y}-{m}, días: {days}\")\n",
    "\n",
    "    # 1) Descarga cada estadística en su NC propio\n",
    "    tmp_files = []\n",
    "    for suffix, daily_stat, var, freq in STATS:\n",
    "        fp = os.path.join(TMP_DIR, f\"{y}{m}_{suffix}.nc\")\n",
    "        print(f\"  ▷ Descargando {suffix}… \", end=\"\", flush=True)\n",
    "        client.retrieve(\n",
    "            \"derived-era5-single-levels-daily-statistics\",\n",
    "            {\n",
    "                \"product_type\":    \"reanalysis\",\n",
    "                \"format\":          \"netcdf\",\n",
    "                \"variable\":        [var],\n",
    "                \"year\":            [y],\n",
    "                \"month\":           [m],\n",
    "                \"day\":             days,\n",
    "                \"daily_statistic\": daily_stat,\n",
    "                \"frequency\":       freq,\n",
    "                \"time_zone\":       \"utc+00:00\",\n",
    "                \"area\":            AREA,\n",
    "            },\n",
    "            fp\n",
    "        )\n",
    "        print(\"OK\")\n",
    "        tmp_files.append(fp)\n",
    "\n",
    "    # 2) Merge dinámico y renombrado implícito\n",
    "    datasets = []\n",
    "    for fp in tmp_files:\n",
    "        ds = xr.open_dataset(fp, engine=\"netcdf4\")\n",
    "        # detecta la única variable data_var\n",
    "        data_vars = [v for v in ds.data_vars if v not in (\"latitude\",\"longitude\") \n",
    "                     and not v.lower().endswith(\"time\")]\n",
    "        if len(data_vars) != 1:\n",
    "            raise RuntimeError(f\"{fp} -> variables inesperadas: {data_vars}\")\n",
    "        # renombrar al sufijo (parte tras el guion bajo en el nombre de fichero)\n",
    "        suffix = os.path.basename(fp).split(\"_\")[-1].replace(\".nc\",\"\")\n",
    "        ds = ds.rename({data_vars[0]: suffix})\n",
    "        datasets.append(ds)\n",
    "\n",
    "    combined = xr.merge(datasets)\n",
    "    combined.attrs.clear()\n",
    "\n",
    "    # 3) Guardar NC combinado\n",
    "    out_fp = os.path.join(TMP_DIR, f\"{y}{m}_daily_stats.nc\")\n",
    "    print(f\"  ▷ Guardando combinado → {out_fp}\")\n",
    "    combined.to_netcdf(out_fp, format=\"NETCDF4\")\n",
    "\n",
    "    # 4) Insertar en SQLite\n",
    "    print(f\"  ▷ Insertando en SQLite desde {out_fp}\")\n",
    "    insert_nc_to_sql(out_fp, DB_PATH, table=\"climate_stats\")\n",
    "\n",
    "    # 5) Limpiar temporales individuales\n",
    "    for fp in tmp_files:\n",
    "        try: os.remove(fp)\n",
    "        except: pass\n",
    "    # (opcional) borrar también el combinado:\n",
    "    # try: os.remove(out_fp)\n",
    "    # except: pass\n",
    "\n",
    "print(\"\\n¡Proceso completado!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "088a69e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(0, 'date', 'TEXT', 0, None, 1), (1, 'latitude', 'REAL', 0, None, 2), (2, 'longitude', 'REAL', 0, None, 3), (3, 't2m_min', 'REAL', 0, None, 0), (4, 't2m_max', 'REAL', 0, None, 0), (5, 'u10_mean', 'REAL', 0, None, 0), (6, 'v10_mean', 'REAL', 0, None, 0), (7, 'wind_speed', 'REAL', 0, None, 0)]\n"
     ]
    }
   ],
   "source": [
    "import sqlite3\n",
    "conn = sqlite3.connect(DB_PATH)\n",
    "cursor = conn.cursor()\n",
    "cursor.execute(\"PRAGMA table_info(climate_stats)\")\n",
    "print(cursor.fetchall())\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366e9978",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'d9af180c-f7f8-4a2e-8029-09018fb8c920'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CDS_KEY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "210874fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "conn.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "licitacion",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
